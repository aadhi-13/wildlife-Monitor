{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Trimmin Zero voice\n"
      ],
      "metadata": {
        "id": "VNhUACLk6J6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "from scipy.io.wavfile import write\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "# NOTE: This assumes your long audio file is already on your local machine (Pop!_OS)\n",
        "INPUT_AUDIO_PATH = os.path.expanduser('/content/drive/MyDrive/Dataset 3.0/bearfull.wav')\n",
        "OUTPUT_AUDIO_PATH = os.path.expanduser('/content/drive/MyDrive/Dataset 3.0/bearfull(mod).wav')\n",
        "os.makedirs(os.path.dirname(OUTPUT_AUDIO_PATH), exist_ok=True)\n",
        "\n",
        "\n",
        "# --- Silence Detection Parameters ---\n",
        "# The threshold (in dBFS) below which to consider silence.\n",
        "# -40 dBFS is a good starting point for environmental audio. Adjust based on your mic/noise floor.\n",
        "SILENCE_THRESHOLD_DB = -40\n",
        "\n",
        "# The minimum length of silence (in ms) that we want to *treat as a break*.\n",
        "# We want to remove silence ONLY if it's longer than 4 seconds (4000 ms).\n",
        "MIN_SILENCE_LEN_MS = 4000\n",
        "\n",
        "\n",
        "def trim_long_silence(input_path, output_path, silence_thresh_db, min_silence_len_ms):\n",
        "    \"\"\"\n",
        "    Identifies and removes segments of silence longer than min_silence_len_ms.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the audio file (Pydub handles WAV format efficiently)\n",
        "        print(f\"Loading audio from: {input_path}\")\n",
        "        audio = AudioSegment.from_file(input_path, format=\"wav\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load audio file. {e}\")\n",
        "        return\n",
        "\n",
        "    # 1. Split the audio whenever silence longer than the threshold is found\n",
        "    # We set keep_silence=0 to remove ALL detected silence first.\n",
        "    non_silent_chunks = split_on_silence(\n",
        "        audio,\n",
        "        min_silence_len=min_silence_len_ms,\n",
        "        silence_thresh=silence_thresh_db,\n",
        "        keep_silence=0  # Remove all silence initially\n",
        "    )\n",
        "\n",
        "    if not non_silent_chunks:\n",
        "        print(\"WARNING: No non-silent audio detected. File may be entirely silence.\")\n",
        "        return\n",
        "\n",
        "    # 2. Recombine the chunks, adding back a fixed amount of silence (e.g., 2 seconds)\n",
        "    # This ensures short pauses remain, but any silence longer than 4s is replaced by 2s.\n",
        "\n",
        "    # Define a 2-second silence chunk to put between the active audio chunks\n",
        "    fixed_pause = AudioSegment.silent(duration=2000)\n",
        "\n",
        "    # Initialize the final audio with the first chunk\n",
        "    processed_audio = non_silent_chunks[0]\n",
        "\n",
        "    # Loop through the rest of the chunks and add the fixed pause before each one\n",
        "    for chunk in non_silent_chunks[1:]:\n",
        "        processed_audio += fixed_pause\n",
        "        processed_audio += chunk\n",
        "\n",
        "    # 3. Export the final audio\n",
        "    print(f\"Original duration: {len(audio) / 1000:.2f}s\")\n",
        "    print(f\"Trimmed duration: {len(processed_audio) / 1000:.2f}s\")\n",
        "\n",
        "    processed_audio.export(output_path, format=\"wav\")\n",
        "    print(f\"\\nSuccessfully saved trimmed audio to: {output_path}\")\n",
        "\n",
        "\n",
        "# --- Execute the Function ---\n",
        "trim_long_silence(\n",
        "    INPUT_AUDIO_PATH,\n",
        "    OUTPUT_AUDIO_PATH,\n",
        "    SILENCE_THRESHOLD_DB,\n",
        "    MIN_SILENCE_LEN_MS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-4IzhTu6P5o",
        "outputId": "580e580b-b573-4768-a489-f91dcf3b42cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading audio from: /content/drive/MyDrive/Dataset 3.0/bearfull.wav\n",
            "Original duration: 863.62s\n",
            "Trimmed duration: 758.40s\n",
            "\n",
            "Successfully saved trimmed audio to: /content/drive/MyDrive/Dataset 3.0/bearfull(mod).wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting Large audio into 4 seconds chunk"
      ],
      "metadata": {
        "id": "uICj54SC8tdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Setup and Configuration ---\n",
        "# You need to run 'pip install pydub' if you haven't already.\n",
        "\n",
        "# Mount Google Drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define paths (UPDATE THESE PATHS to your specific file names and locations)\n",
        "# This should be the long audio file created from the concatenation step\n",
        "LONG_AUDIO_FILE = '/content/drive/MyDrive/Dataset 3.0/bggfull.wav'\n",
        "\n",
        "# This is the new folder where all 4-second chunks will be saved\n",
        "CHUNKS_OUTPUT_FOLDER = '/content/drive/MyDrive/Dataset 3.0/Background'\n",
        "\n",
        "# Define the chunk size\n",
        "CHUNK_LENGTH_MS = 4000  # 4 seconds in milliseconds\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(CHUNKS_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- Function to Split Audio ---\n",
        "def split_audio_into_chunks(input_file, output_dir, chunk_length_ms):\n",
        "    \"\"\"\n",
        "    Loads a long audio file, splits it into fixed-length chunks, and saves them.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the audio file (Pydub automatically detects the format)\n",
        "        print(f\"Loading audio file: {input_file}...\")\n",
        "        audio = AudioSegment.from_file(input_file)\n",
        "\n",
        "        file_name_base = os.path.splitext(os.path.basename(input_file))[0]\n",
        "\n",
        "        # Calculate total duration and number of full chunks\n",
        "        total_duration = len(audio)\n",
        "        num_chunks = int(total_duration / chunk_length_ms)\n",
        "\n",
        "        print(f\"Total duration: {total_duration / 1000} seconds. Expected full chunks: {num_chunks}\")\n",
        "\n",
        "        # Loop through and export full-length chunks\n",
        "        for i in range(num_chunks):\n",
        "            start_time = i * chunk_length_ms\n",
        "            end_time = start_time + chunk_length_ms\n",
        "\n",
        "            # Slice the audio segment\n",
        "            chunk = audio[start_time:end_time]\n",
        "\n",
        "            # Define the output filename with padded index (e.g., chunk_0001.wav)\n",
        "            output_filename = f\"{file_name_base}_chunk_{i:04d}.wav\"\n",
        "            output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "            # Export the chunk (using WAV for lossless quality)\n",
        "            chunk.export(output_path, format=\"wav\")\n",
        "\n",
        "        print(f\"\\nSuccessfully created and saved {num_chunks} chunks to {output_dir}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Input file not found at {input_file}. Please check the path.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during splitting: {e}\")\n",
        "\n",
        "# --- Execute ---\n",
        "split_audio_into_chunks(LONG_AUDIO_FILE, CHUNKS_OUTPUT_FOLDER, CHUNK_LENGTH_MS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDjwzN4fwxTs",
        "outputId": "ac3d05e7-3f55-42e9-8391-0363640192ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading audio file: /content/drive/MyDrive/Dataset 3.0/bggfull.wav...\n",
            "Total duration: 259.325 seconds. Expected full chunks: 64\n",
            "\n",
            "Successfully created and saved 64 chunks to /content/drive/MyDrive/Dataset 3.0/Background\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking number of dataset\n"
      ],
      "metadata": {
        "id": "Kd1FODai88mH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# # 1. Ensure Google Drive is mounted\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 2. Define the path to the folder you want to count\n",
        "# Replace 'Your_Folder_Name' with the actual path (e.g., 'dataset/SAS-KIIT')\n",
        "FOLDER_PATH = '/content/drive/MyDrive/Dataset 3.0/Wild Boar'\n",
        "\n",
        "# 3. Use os.listdir to get a list of all contents (files and subfolders)\n",
        "try:\n",
        "    all_items = os.listdir(FOLDER_PATH)\n",
        "\n",
        "    # 4. Filter the list to count only files (not directories)\n",
        "    file_count = 0\n",
        "    for item in all_items:\n",
        "        # Construct the full path for os.path.isfile check\n",
        "        full_path = os.path.join(FOLDER_PATH, item)\n",
        "        if os.path.isfile(full_path):\n",
        "            file_count += 1\n",
        "\n",
        "    # You can also count all items (files and folders) if you prefer:\n",
        "    # total_items = len(all_items)\n",
        "\n",
        "    print(f\"Folder to check: {FOLDER_PATH}\")\n",
        "    print(f\"Total number of files in the folder: {file_count}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The folder was not found at the specified path: {FOLDER_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8uaKzmSp-kv",
        "outputId": "c5359ab2-eb2b-42f5-f591-f3fb313f6fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder to check: /content/drive/MyDrive/Dataset 3.0/Wild Boar\n",
            "Total number of files in the folder: 93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Meta data creation\n"
      ],
      "metadata": {
        "id": "Vw7WnNNfeDv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Setup and Configuration ---\n",
        "\n",
        "# # 1. Mount Google Drive (if running in a new session)\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 2. Define the path containing ALL your processed audio folders\n",
        "# Assuming all four folders (Background, Elephant, Tiger, Wild_Boar) are inside 'Sound_2.0'\n",
        "BASE_AUDIO_DIR = '/content/drive/MyDrive/Dataset 3.0'\n",
        "\n",
        "# 3. Define the output file path for your new metadata CSV\n",
        "OUTPUT_METADATA_PATH = os.path.join(BASE_AUDIO_DIR, 'final_training_metadata.csv')\n",
        "\n",
        "# List of your class folders (must match the folder names exactly)\n",
        "CLASS_FOLDERS = ['Background', 'Bear', 'Deer','Elephant', 'Monkey', 'Peacock', 'Tiger', 'Wild Boar']\n",
        "\n",
        "# --- Function to Generate Metadata ---\n",
        "\n",
        "def generate_metadata(base_dir, class_folders):\n",
        "    \"\"\"\n",
        "    Scans specified folders, collects file paths and assigns class labels.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "\n",
        "    print(f\"Starting scan of folders in: {base_dir}\")\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        folder_path = os.path.join(base_dir, class_name)\n",
        "        print(f\"Processing folder: {class_name}...\")\n",
        "\n",
        "        if not os.path.isdir(folder_path):\n",
        "            print(f\"WARNING: Folder not found at {folder_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Use os.listdir to get all files in the class folder\n",
        "        for file_name in os.listdir(folder_path):\n",
        "            if file_name.lower().endswith('.wav'):  # Ensure it's a WAV file\n",
        "\n",
        "                # We need the relative path for easy use later\n",
        "                relative_path = os.path.join(class_name, file_name)\n",
        "\n",
        "                data.append({\n",
        "                    'file_path': relative_path,\n",
        "                    'class_name': class_name\n",
        "                })\n",
        "\n",
        "    # 4. Convert the collected data to a DataFrame\n",
        "    if not data:\n",
        "        print(\"No audio files were found. Check BASE_AUDIO_DIR and folder names.\")\n",
        "        return None\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f\"\\nScan Complete. Total records found: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "# --- Execute and Save ---\n",
        "\n",
        "# 1. Generate the DataFrame\n",
        "metadata_df = generate_metadata(BASE_AUDIO_DIR, CLASS_FOLDERS)\n",
        "\n",
        "# 2. Save the DataFrame to a CSV file\n",
        "if metadata_df is not None:\n",
        "    metadata_df.to_csv(OUTPUT_METADATA_PATH, index=False)\n",
        "    print(f\"Metadata saved successfully to: {OUTPUT_METADATA_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlpZcqQDeHSb",
        "outputId": "15f30a93-95d0-463d-d3fe-553c4f3344c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scan of folders in: /content/drive/MyDrive/Dataset 3.0\n",
            "Processing folder: Background...\n",
            "Processing folder: Bear...\n",
            "Processing folder: Deer...\n",
            "Processing folder: Elephant...\n",
            "Processing folder: Monkey...\n",
            "Processing folder: Peacock...\n",
            "Processing folder: Tiger...\n",
            "Processing folder: Wild Boar...\n",
            "\n",
            "Scan Complete. Total records found: 8401\n",
            "Metadata saved successfully to: /content/drive/MyDrive/Dataset 3.0/final_training_metadata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating weights for each class"
      ],
      "metadata": {
        "id": "UUG69H8Fn3CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Your current dataset distribution (based on your input)\n",
        "class_counts = {\n",
        "    'Bear': 1119,\n",
        "    'Deer': 986,\n",
        "    'Elephant': 1075,\n",
        "    'Monkey': 1030,\n",
        "    'Peacock': 1007,\n",
        "    'Tiger': 1152,\n",
        "    'Wild_Boar': 998,\n",
        "    'Background': 1034\n",
        "}\n",
        "\n",
        "# 1. Prepare data for the utility function\n",
        "# Get the class names and sample counts\n",
        "class_names = np.array(list(class_counts.keys()))\n",
        "y_integers = np.array([\n",
        "    name for name, count in class_counts.items() for _ in range(count)\n",
        "])\n",
        "\n",
        "# 2. Calculate the weights\n",
        "# 'balanced' mode automatically sets weights inversely proportional to class frequencies.\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_integers),\n",
        "    y=y_integers\n",
        ")\n",
        "\n",
        "# 3. Convert to a dictionary mapping integer index (0, 1, 2...) to the weight\n",
        "class_weights = dict(enumerate(weights))\n",
        "\n",
        "# Print the results for inspection\n",
        "print(\"--- Class Weights Calculated ---\")\n",
        "print(\"Class Name : Weight\")\n",
        "for idx, weight in class_weights.items():\n",
        "    print(f\"{class_names[idx]} (Count={class_counts[class_names[idx]]}) : {weight:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glj9v6aboDhP",
        "outputId": "f205a5a6-e6f3-4c62-da6c-70e856ce6946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Class Weights Calculated ---\n",
            "Class Name : Weight\n",
            "Bear (Count=1119) : 1.0156\n",
            "Deer (Count=986) : 0.9384\n",
            "Elephant (Count=1075) : 1.0650\n",
            "Monkey (Count=1030) : 0.9769\n",
            "Peacock (Count=1007) : 1.0195\n",
            "Tiger (Count=1152) : 1.0428\n",
            "Wild_Boar (Count=998) : 0.9116\n",
            "Background (Count=1034) : 1.0522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetaure Extraction\n"
      ],
      "metadata": {
        "id": "mDnFkzDkt5tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration Constants (MUST MATCH TRAINING SETUP) ---\n",
        "TARGET_SR = 22050        # Standard sample rate\n",
        "TARGET_DURATION_SEC = 4.0 # Length of the audio segment\n",
        "TARGET_SAMPLES = int(TARGET_DURATION_SEC * TARGET_SR)\n",
        "N_MFCC = 40              # Number of MFCC features (40 is standard)\n",
        "\n",
        "# --- Path Configuration (VERIFY THESE PATHS) ---\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "BASE_AUDIO_DIR = '/content/drive/MyDrive/Dataset 3.0'\n",
        "METADATA_FILE = os.path.join(BASE_AUDIO_DIR, 'final_training_metadata.csv')\n",
        "\n",
        "# Define a new path to save the processed MFCC data\n",
        "PROCESSED_MFCC_DIR = os.path.join(BASE_AUDIO_DIR, 'processed_mfcc_data')\n",
        "os.makedirs(PROCESSED_MFCC_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- 1. MFCC Extraction Function (Handles Padding) ---\n",
        "def extract_mfcc_sequence(audio_path, n_mfcc=N_MFCC, target_sr=TARGET_SR, target_samples=TARGET_SAMPLES):\n",
        "    \"\"\"\n",
        "    Loads audio, pads it to the target duration (4s), and extracts MFCC features.\n",
        "    Returns: A 2D array: (Time Steps, Features)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=target_sr, mono=True)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Pad the audio array to the target number of samples (4 seconds)\n",
        "    y_fixed = librosa.util.fix_length(y, size=target_samples, mode='constant')\n",
        "\n",
        "    # Extract MFCC features\n",
        "    mfccs = librosa.feature.mfcc(y=y_fixed, sr=sr, n_mfcc=n_mfcc)\n",
        "\n",
        "    # Transpose to get the required LSTM input shape: (Time Steps, Features)\n",
        "    return mfccs.T\n",
        "\n",
        "\n",
        "# --- 2. Data Processing Loop ---\n",
        "df = pd.read_csv(METADATA_FILE)\n",
        "\n",
        "mfcc_sequences = []\n",
        "labels = []\n",
        "processed_files_count = 0\n",
        "\n",
        "print(f\"\\nStarting MFCC Feature Extraction (n_mfcc={N_MFCC})...\")\n",
        "\n",
        "# Iterate over the metadata DataFrame to load files\n",
        "for index, row in df.iterrows():\n",
        "    # Construct the full path\n",
        "    relative_path = row['file_path']\n",
        "    audio_path = os.path.join(BASE_AUDIO_DIR, relative_path)\n",
        "    label = row['class_name']\n",
        "\n",
        "    # Extract the MFCC sequence\n",
        "    mfcc_sequence = extract_mfcc_sequence(audio_path)\n",
        "\n",
        "    if mfcc_sequence is not None:\n",
        "        mfcc_sequences.append(mfcc_sequence)\n",
        "        labels.append(label)\n",
        "        processed_files_count += 1\n",
        "\n",
        "        if processed_files_count % 500 == 0:\n",
        "            print(f\"Processed {processed_files_count} files.\")\n",
        "\n",
        "\n",
        "# --- 3. Final Array Conversion and Saving ---\n",
        "\n",
        "if not mfcc_sequences:\n",
        "    print(\"No MFCC sequences were created. Cannot save.\")\n",
        "else:\n",
        "    # Convert lists to NumPy arrays\n",
        "    X_lstm = np.array(mfcc_sequences)\n",
        "    y_lstm = np.array(labels)\n",
        "\n",
        "    # Save the arrays to Google Drive\n",
        "    np.save(os.path.join(PROCESSED_MFCC_DIR, 'X_mfccs.npy'), X_lstm)\n",
        "    np.save(os.path.join(PROCESSED_MFCC_DIR, 'y_labels.npy'), y_lstm)\n",
        "\n",
        "    print(f\"\\nFinished processing. Total sequences created: {len(X_lstm)}\")\n",
        "    print(f\"Final LSTM Input Shape (X_lstm): {X_lstm.shape} (Samples, Time Steps, Features)\")\n",
        "    print(f\"\\nSuccessfully saved X_mfccs.npy and y_labels.npy to: {PROCESSED_MFCC_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw21BfGpt5F5",
        "outputId": "25023071-d828-4726-8e0d-e79ab4f5cc06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting MFCC Feature Extraction (n_mfcc=40)...\n",
            "Processed 500 files.\n",
            "Processed 1000 files.\n",
            "Processed 1500 files.\n",
            "Processed 2000 files.\n",
            "Processed 2500 files.\n",
            "Processed 3000 files.\n",
            "Processed 3500 files.\n",
            "Processed 4000 files.\n",
            "Processed 4500 files.\n",
            "Processed 5000 files.\n",
            "Processed 5500 files.\n",
            "Processed 6000 files.\n",
            "Processed 6500 files.\n",
            "Processed 7000 files.\n",
            "Processed 7500 files.\n",
            "Processed 8000 files.\n",
            "\n",
            "Finished processing. Total sequences created: 8401\n",
            "Final LSTM Input Shape (X_lstm): (8401, 173, 40) (Samples, Time Steps, Features)\n",
            "\n",
            "Successfully saved X_mfccs.npy and y_labels.npy to: /content/drive/MyDrive/Dataset 3.0/processed_mfcc_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data splitting\n"
      ],
      "metadata": {
        "id": "PsJIpkSxw4Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration (Must match saved files) ---\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "BASE_AUDIO_DIR = '/content/drive/MyDrive/Dataset 3.0'\n",
        "PROCESSED_MFCC_DIR = os.path.join(BASE_AUDIO_DIR, 'processed_mfcc_data')\n",
        "LABELS_NPY_PATH = os.path.join(PROCESSED_MFCC_DIR, 'y_labels.npy')\n",
        "X_NPY_PATH = os.path.join(PROCESSED_MFCC_DIR, 'X_mfccs.npy')\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "try:\n",
        "    X_lstm = np.load(X_NPY_PATH)\n",
        "    y_lstm = np.load(LABELS_NPY_PATH)\n",
        "    print(\"Data loaded successfully.\")\n",
        "    print(f\"Initial X shape: {X_lstm.shape}, Initial y shape: {y_lstm.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: MFCC files not found. Please ensure files are saved and paths are correct.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Encoding and Splitting ---\n",
        "\n",
        "# A. Encode string labels into integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_lstm)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "print(f\"Number of unique classes found: {num_classes}\")\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "# B. Calculate Class Weights (Crucial for imbalanced data)\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_encoded),\n",
        "    y=y_encoded\n",
        ")\n",
        "class_weights = dict(enumerate(weights))\n",
        "\n",
        "# C. Split the data (70% Train, 15% Val, 15% Test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X_lstm, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# D. One-hot encode the integer labels\n",
        "# Explicitly pass the number of classes to to_categorical\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
        "y_val_one_hot = to_categorical(y_val, num_classes=num_classes)\n",
        "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "# --- 3. Final Reshaping for CRNN (From MFCC to CNN-compatible) ---\n",
        "\n",
        "# CRNN (TimeDistributed CNN part) expects a 4D array: (Samples, Time Steps, Height, 1)\n",
        "# Your X_lstm is currently 3D: (Samples, Time Steps, Features)\n",
        "\n",
        "# Reshape X arrays: Add a channel dimension of 1 for the CNN filter\n",
        "# This tells the CNN that the \"height\" is the feature dimension (40 MFCCs)\n",
        "# and the \"width\" is the time step dimension.\n",
        "X_train_crnn = np.expand_dims(X_train, axis=-1)\n",
        "X_val_crnn = np.expand_dims(X_val, axis=-1)\n",
        "X_test_crnn = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "\n",
        "# Final check\n",
        "print(\"\\n--- Data Preparation Summary ---\")\n",
        "print(f\"CRNN Input Shape (X_train): {X_train_crnn.shape} (Samples, Time Steps, Features, 1)\")\n",
        "print(f\"Training Samples: {X_train_crnn.shape[0]}\")\n",
        "print(f\"Validation Samples: {X_val_crnn.shape[0]}\")\n",
        "print(f\"Number of classes used for one-hot encoding: {num_classes}\")\n",
        "print(\"Class weights calculated and ready for model.fit()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0XL29sDw7tj",
        "outputId": "82e4e9ab-630e-4536-d09d-6e99053cdde3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "Initial X shape: (8401, 173, 40), Initial y shape: (8401,)\n",
            "Number of unique classes found: 8\n",
            "Class names: ['Background' 'Bear' 'Deer' 'Elephant' 'Monkey' 'Peacock' 'Tiger'\n",
            " 'Wild Boar']\n",
            "\n",
            "--- Data Preparation Summary ---\n",
            "CRNN Input Shape (X_train): (5880, 173, 40, 1) (Samples, Time Steps, Features, 1)\n",
            "Training Samples: 5880\n",
            "Validation Samples: 1260\n",
            "Number of classes used for one-hot encoding: 8\n",
            "Class weights calculated and ready for model.fit()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Architecture\n"
      ],
      "metadata": {
        "id": "0eQ_ipa4xgKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GRU, Dense, Dropout, Flatten, Reshape\n",
        "\n",
        "# --- Assume the following variables are defined from the previous code execution: ---\n",
        "# X_train_crnn (Input Data, shape: Samples, Time Steps, Features, 1)\n",
        "# num_classes (Total number of unique classes)\n",
        "# class_weights (Calculated weights for imbalance)\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "# Get the input shape: (Time Steps, Features, 1)\n",
        "input_shape = X_train_crnn.shape[1:]\n",
        "print(f\"Model Input Shape (Time Steps, Features, 1): {input_shape}\")\n",
        "\n",
        "\n",
        "# --- Build the CRNN Model ---\n",
        "model_crnn = Sequential([\n",
        "    tf.keras.Input(shape=input_shape),\n",
        "\n",
        "    # 1. Convolutional Block (Extracts local frequency/time features)\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # 2. Reshape Layer (Crucial for connecting CNN to GRU)\n",
        "    # This reshapes the 4D output of the CNN into a 3D sequence (Time Steps, New Features)\n",
        "    # We flatten the frequency/channel dimensions to create the sequence features.\n",
        "\n",
        "    # Calculate the remaining spatial dimensions after max pooling:\n",
        "    # Time dimension: input_shape[0] / 4 (since we used 2x2 pooling twice)\n",
        "    # Feature dimension: input_shape[1] / 4 * 64 (last ConvD layer output)\n",
        "\n",
        "    # We use the current shape and only flatten the feature/channel dimensions\n",
        "    # to maintain the TIME STEP sequence length.\n",
        "\n",
        "    # Keras Flatten() automatically flattens all dimensions except the batch size.\n",
        "    # We must calculate the new feature count for the GRU.\n",
        "\n",
        "    # The output of the last MaxPooling2D will be (None, Time_new, Feature_new, 64)\n",
        "    # We flatten the (Feature_new, 64) dimensions into a single vector per Time Step.\n",
        "\n",
        "    tf.keras.layers.Permute((2, 1, 3)), # Permute to put Time dimension first if needed,\n",
        "                                        # but sticking with Conv2D on (Time, Freq) for simplicity.\n",
        "\n",
        "    # Calculate the shape after the last MaxPooling (approximate values used below)\n",
        "    # The output is still a time-sequence, but the features are compressed.\n",
        "    # We use Reshape or TimeDistributed(Flatten) for this step.\n",
        "\n",
        "    # Simplified approach: Flatten and then reshape into a sequence for GRU.\n",
        "    tf.keras.layers.Reshape((-1, 64)), # Assumes a final sequence length of -1 (automatic) and 64 features per step.\n",
        "\n",
        "    # --- Simplified Classifier for immediate fix ---\n",
        "\n",
        "    # 3. Recurrent Block (Analyzes the feature sequence over time)\n",
        "    GRU(128, return_sequences=True, activation='tanh', name='gru_1'),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    GRU(128, activation='tanh', name='gru_2'), # Last GRU returns a single summary vector\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # 4. Final Dense layers\n",
        "    Dense(128, activation='relu', name='dense_1'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Output layer (must match num_classes exactly)\n",
        "    Dense(num_classes, activation='softmax', name='output_layer')\n",
        "])\n",
        "\n",
        "# 5. Compile the Model\n",
        "model_crnn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_crnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "ceYbHBP4xiiX",
        "outputId": "9d4c3ae3-f891-44fa-fd3e-c4d9eef46b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Input Shape (Time Steps, Features, 1): (173, 40, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m173\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m173\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m36,992\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ permute (\u001b[38;5;33mPermute\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m860\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m860\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m74,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m860\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m99,072\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m1,032\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">173</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">173</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,992</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ permute (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">860</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">860</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">74,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">860</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m229,064\u001b[0m (894.78 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">229,064</span> (894.78 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m228,744\u001b[0m (893.53 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">228,744</span> (893.53 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m320\u001b[0m (1.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> (1.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the CRNN Model"
      ],
      "metadata": {
        "id": "ylVhvMB49H0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "# Ensure X_train_crnn, y_train_one_hot, X_val_crnn, y_val_one_hot,\n",
        "# model_crnn, class_weights, X_test_crnn, y_test_one_hot are available.\n",
        "\n",
        "# --- Configuration for Saving Paths ---\n",
        "BASE_MODELS_DIR = '/content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/'\n",
        "FINAL_MODEL_PATH = os.path.join(BASE_MODELS_DIR, 'wildlife_crnn_FINAL.h5')\n",
        "CHECKPOINT_FILEPATH = os.path.join(BASE_MODELS_DIR, 'crnn_checkpoint_{epoch:02d}.h5')\n",
        "\n",
        "# Ensure the checkpoint directory exists\n",
        "os.makedirs(os.path.dirname(CHECKPOINT_FILEPATH), exist_ok=True)\n",
        "print(f\"Checkpoints will be saved to: {os.path.dirname(BASE_MODELS_DIR)}\")\n",
        "\n",
        "# --- Create Checkpoint Callback ---\n",
        "# This callback saves the model after every epoch.\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=CHECKPOINT_FILEPATH,\n",
        "    save_weights_only=False,  # Saves the full model (architecture + weights)\n",
        "    save_freq='epoch',        # Saves after every epoch\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- Train the Model ---\n",
        "print(\"\\n--- Starting CRNN Training with Class Weighting ---\")\n",
        "\n",
        "history = model_crnn.fit(\n",
        "    X_train_crnn, # Use the reshaped CRNN training data\n",
        "    y_train_one_hot,\n",
        "    epochs=75,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val_crnn, y_val_one_hot),\n",
        "    class_weight=class_weights, # Applying the calculated weights\n",
        "    callbacks=[checkpoint_callback] # Add the checkpoint callback here\n",
        ")\n",
        "\n",
        "print(\"\\n--- CRNN Model Training Complete ---\")\n",
        "\n",
        "# --- Evaluate and Save Final Model ---\n",
        "\n",
        "# 1. Evaluate the model on the test set\n",
        "# Assuming X_test_crnn and y_test_one_hot are available.\n",
        "test_loss, test_acc = model_crnn.evaluate(X_test_crnn, y_test_one_hot, verbose=2)\n",
        "\n",
        "print(\"--------------------------------------------------------------------\")\n",
        "print(f\"Final Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
        "print(\"--------------------------------------------------------------------\")\n",
        "\n",
        "# 2. Save the final trained model explicitly (best practice)\n",
        "os.makedirs(os.path.dirname(FINAL_MODEL_PATH), exist_ok=True)\n",
        "model_crnn.save(FINAL_MODEL_PATH)\n",
        "print(f\"\\nFinal model saved explicitly at: {FINAL_MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37X1EmsMxvEx",
        "outputId": "56a3c5d8-c345-4c2a-e112-fc8b4b785638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoints will be saved to: /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints\n",
            "\n",
            "--- Starting CRNN Training with Class Weighting ---\n",
            "Epoch 1/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.2480 - loss: 1.9248\n",
            "Epoch 1: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_01.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 118ms/step - accuracy: 0.2489 - loss: 1.9231 - val_accuracy: 0.2746 - val_loss: 2.2859\n",
            "Epoch 2/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.5163 - loss: 1.3453\n",
            "Epoch 2: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_02.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.5171 - loss: 1.3436 - val_accuracy: 0.5714 - val_loss: 1.2926\n",
            "Epoch 3/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.7713 - loss: 0.7680\n",
            "Epoch 3: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_03.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 0.7714 - loss: 0.7674 - val_accuracy: 0.7873 - val_loss: 0.7570\n",
            "Epoch 4/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8351 - loss: 0.5895\n",
            "Epoch 4: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_04.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 0.8351 - loss: 0.5892 - val_accuracy: 0.8833 - val_loss: 0.4394\n",
            "Epoch 5/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8744 - loss: 0.4470\n",
            "Epoch 5: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_05.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.8744 - loss: 0.4471 - val_accuracy: 0.8825 - val_loss: 0.4214\n",
            "Epoch 6/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8899 - loss: 0.3993\n",
            "Epoch 6: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_06.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.8899 - loss: 0.3993 - val_accuracy: 0.9111 - val_loss: 0.3124\n",
            "Epoch 7/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8934 - loss: 0.3709\n",
            "Epoch 7: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_07.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.8935 - loss: 0.3710 - val_accuracy: 0.9111 - val_loss: 0.3346\n",
            "Epoch 8/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9107 - loss: 0.3115\n",
            "Epoch 8: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_08.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.9107 - loss: 0.3114 - val_accuracy: 0.9159 - val_loss: 0.2949\n",
            "Epoch 9/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9186 - loss: 0.2806\n",
            "Epoch 9: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_09.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.9186 - loss: 0.2805 - val_accuracy: 0.9087 - val_loss: 0.3084\n",
            "Epoch 10/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9194 - loss: 0.2724\n",
            "Epoch 10: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_10.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9194 - loss: 0.2723 - val_accuracy: 0.9302 - val_loss: 0.2049\n",
            "Epoch 11/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9375 - loss: 0.2084\n",
            "Epoch 11: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_11.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9374 - loss: 0.2086 - val_accuracy: 0.9317 - val_loss: 0.2174\n",
            "Epoch 12/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9373 - loss: 0.2145\n",
            "Epoch 12: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_12.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9373 - loss: 0.2146 - val_accuracy: 0.9119 - val_loss: 0.2694\n",
            "Epoch 13/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9340 - loss: 0.2239\n",
            "Epoch 13: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_13.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.9340 - loss: 0.2237 - val_accuracy: 0.9357 - val_loss: 0.2078\n",
            "Epoch 14/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9384 - loss: 0.1940\n",
            "Epoch 14: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_14.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9384 - loss: 0.1941 - val_accuracy: 0.9389 - val_loss: 0.1795\n",
            "Epoch 15/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9449 - loss: 0.1791\n",
            "Epoch 15: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_15.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9449 - loss: 0.1790 - val_accuracy: 0.9437 - val_loss: 0.1845\n",
            "Epoch 16/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9462 - loss: 0.1869\n",
            "Epoch 16: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_16.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9462 - loss: 0.1866 - val_accuracy: 0.9452 - val_loss: 0.1957\n",
            "Epoch 17/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9484 - loss: 0.1474\n",
            "Epoch 17: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_17.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 118ms/step - accuracy: 0.9484 - loss: 0.1476 - val_accuracy: 0.9460 - val_loss: 0.1974\n",
            "Epoch 18/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9522 - loss: 0.1553\n",
            "Epoch 18: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_18.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.9522 - loss: 0.1552 - val_accuracy: 0.9294 - val_loss: 0.2129\n",
            "Epoch 19/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9529 - loss: 0.1568\n",
            "Epoch 19: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_19.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9529 - loss: 0.1567 - val_accuracy: 0.9429 - val_loss: 0.1791\n",
            "Epoch 20/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9614 - loss: 0.1110\n",
            "Epoch 20: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_20.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9614 - loss: 0.1111 - val_accuracy: 0.9484 - val_loss: 0.1579\n",
            "Epoch 21/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9670 - loss: 0.1096\n",
            "Epoch 21: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_21.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9669 - loss: 0.1099 - val_accuracy: 0.9476 - val_loss: 0.1883\n",
            "Epoch 22/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9527 - loss: 0.1421\n",
            "Epoch 22: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_22.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9527 - loss: 0.1419 - val_accuracy: 0.9500 - val_loss: 0.1545\n",
            "Epoch 23/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9661 - loss: 0.0972\n",
            "Epoch 23: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_23.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.9661 - loss: 0.0973 - val_accuracy: 0.9563 - val_loss: 0.1424\n",
            "Epoch 24/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9632 - loss: 0.1130\n",
            "Epoch 24: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_24.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9632 - loss: 0.1129 - val_accuracy: 0.9595 - val_loss: 0.1355\n",
            "Epoch 25/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9699 - loss: 0.0951\n",
            "Epoch 25: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_25.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9699 - loss: 0.0951 - val_accuracy: 0.9563 - val_loss: 0.1577\n",
            "Epoch 26/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9695 - loss: 0.0940\n",
            "Epoch 26: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_26.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9695 - loss: 0.0939 - val_accuracy: 0.9579 - val_loss: 0.1526\n",
            "Epoch 27/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9759 - loss: 0.0741\n",
            "Epoch 27: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_27.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9758 - loss: 0.0743 - val_accuracy: 0.9516 - val_loss: 0.1443\n",
            "Epoch 28/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9764 - loss: 0.0787\n",
            "Epoch 28: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_28.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.9764 - loss: 0.0788 - val_accuracy: 0.9587 - val_loss: 0.1353\n",
            "Epoch 29/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9748 - loss: 0.0907\n",
            "Epoch 29: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_29.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9748 - loss: 0.0907 - val_accuracy: 0.9524 - val_loss: 0.1600\n",
            "Epoch 30/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9769 - loss: 0.0724\n",
            "Epoch 30: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_30.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9768 - loss: 0.0726 - val_accuracy: 0.9587 - val_loss: 0.1270\n",
            "Epoch 31/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9765 - loss: 0.0645\n",
            "Epoch 31: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_31.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9764 - loss: 0.0646 - val_accuracy: 0.9571 - val_loss: 0.1423\n",
            "Epoch 32/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9807 - loss: 0.0663\n",
            "Epoch 32: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_32.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9807 - loss: 0.0662 - val_accuracy: 0.9579 - val_loss: 0.1405\n",
            "Epoch 33/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9858 - loss: 0.0408\n",
            "Epoch 33: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_33.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9858 - loss: 0.0408 - val_accuracy: 0.9563 - val_loss: 0.1599\n",
            "Epoch 34/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9798 - loss: 0.0671\n",
            "Epoch 34: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_34.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9798 - loss: 0.0671 - val_accuracy: 0.9611 - val_loss: 0.1268\n",
            "Epoch 35/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9823 - loss: 0.0504\n",
            "Epoch 35: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_35.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9823 - loss: 0.0504 - val_accuracy: 0.9635 - val_loss: 0.1266\n",
            "Epoch 36/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9870 - loss: 0.0447\n",
            "Epoch 36: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_36.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 121ms/step - accuracy: 0.9870 - loss: 0.0447 - val_accuracy: 0.9675 - val_loss: 0.1284\n",
            "Epoch 37/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9863 - loss: 0.0404\n",
            "Epoch 37: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_37.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 112ms/step - accuracy: 0.9863 - loss: 0.0405 - val_accuracy: 0.9619 - val_loss: 0.1683\n",
            "Epoch 38/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9801 - loss: 0.0635\n",
            "Epoch 38: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_38.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9801 - loss: 0.0634 - val_accuracy: 0.9683 - val_loss: 0.1284\n",
            "Epoch 39/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9789 - loss: 0.0694\n",
            "Epoch 39: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_39.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9789 - loss: 0.0694 - val_accuracy: 0.9540 - val_loss: 0.1691\n",
            "Epoch 40/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9798 - loss: 0.0645\n",
            "Epoch 40: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_40.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 111ms/step - accuracy: 0.9798 - loss: 0.0645 - val_accuracy: 0.9722 - val_loss: 0.1010\n",
            "Epoch 41/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9774 - loss: 0.0705\n",
            "Epoch 41: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_41.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9774 - loss: 0.0705 - val_accuracy: 0.9635 - val_loss: 0.1291\n",
            "Epoch 42/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9894 - loss: 0.0306\n",
            "Epoch 42: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_42.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9894 - loss: 0.0307 - val_accuracy: 0.9587 - val_loss: 0.1874\n",
            "Epoch 43/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9808 - loss: 0.0518\n",
            "Epoch 43: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_43.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9807 - loss: 0.0520 - val_accuracy: 0.9659 - val_loss: 0.1533\n",
            "Epoch 44/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9744 - loss: 0.0821\n",
            "Epoch 44: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_44.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9744 - loss: 0.0820 - val_accuracy: 0.9667 - val_loss: 0.1211\n",
            "Epoch 45/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9890 - loss: 0.0394\n",
            "Epoch 45: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_45.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9889 - loss: 0.0394 - val_accuracy: 0.9683 - val_loss: 0.1269\n",
            "Epoch 46/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9854 - loss: 0.0507\n",
            "Epoch 46: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_46.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9854 - loss: 0.0507 - val_accuracy: 0.9730 - val_loss: 0.1134\n",
            "Epoch 47/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9890 - loss: 0.0320\n",
            "Epoch 47: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_47.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9890 - loss: 0.0320 - val_accuracy: 0.9683 - val_loss: 0.1107\n",
            "Epoch 48/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9945 - loss: 0.0185\n",
            "Epoch 48: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_48.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9944 - loss: 0.0186 - val_accuracy: 0.9730 - val_loss: 0.1269\n",
            "Epoch 49/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9916 - loss: 0.0290\n",
            "Epoch 49: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_49.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 117ms/step - accuracy: 0.9916 - loss: 0.0290 - val_accuracy: 0.9651 - val_loss: 0.1579\n",
            "Epoch 50/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9842 - loss: 0.0578\n",
            "Epoch 50: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_50.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9842 - loss: 0.0577 - val_accuracy: 0.9667 - val_loss: 0.1319\n",
            "Epoch 51/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9894 - loss: 0.0326\n",
            "Epoch 51: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_51.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9894 - loss: 0.0326 - val_accuracy: 0.9603 - val_loss: 0.1729\n",
            "Epoch 52/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9910 - loss: 0.0297\n",
            "Epoch 52: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_52.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.9910 - loss: 0.0297 - val_accuracy: 0.9683 - val_loss: 0.1462\n",
            "Epoch 53/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9889 - loss: 0.0325\n",
            "Epoch 53: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_53.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.9889 - loss: 0.0325 - val_accuracy: 0.9683 - val_loss: 0.1357\n",
            "Epoch 54/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9863 - loss: 0.0435\n",
            "Epoch 54: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_54.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 117ms/step - accuracy: 0.9863 - loss: 0.0436 - val_accuracy: 0.9603 - val_loss: 0.1719\n",
            "Epoch 55/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9914 - loss: 0.0335\n",
            "Epoch 55: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_55.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9914 - loss: 0.0335 - val_accuracy: 0.9635 - val_loss: 0.1401\n",
            "Epoch 56/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9866 - loss: 0.0461\n",
            "Epoch 56: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_56.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.9866 - loss: 0.0460 - val_accuracy: 0.9722 - val_loss: 0.1134\n",
            "Epoch 57/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9903 - loss: 0.0294\n",
            "Epoch 57: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_57.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9903 - loss: 0.0295 - val_accuracy: 0.9619 - val_loss: 0.1638\n",
            "Epoch 58/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9923 - loss: 0.0229\n",
            "Epoch 58: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_58.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.9923 - loss: 0.0229 - val_accuracy: 0.9667 - val_loss: 0.1485\n",
            "Epoch 59/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9955 - loss: 0.0146\n",
            "Epoch 59: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_59.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9955 - loss: 0.0146 - val_accuracy: 0.9690 - val_loss: 0.1455\n",
            "Epoch 60/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9948 - loss: 0.0164\n",
            "Epoch 60: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_60.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9948 - loss: 0.0165 - val_accuracy: 0.9714 - val_loss: 0.1417\n",
            "Epoch 61/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9901 - loss: 0.0308\n",
            "Epoch 61: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_61.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9901 - loss: 0.0308 - val_accuracy: 0.9619 - val_loss: 0.1699\n",
            "Epoch 62/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9850 - loss: 0.0542\n",
            "Epoch 62: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_62.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9849 - loss: 0.0544 - val_accuracy: 0.9690 - val_loss: 0.1317\n",
            "Epoch 63/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9904 - loss: 0.0340\n",
            "Epoch 63: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_63.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.9904 - loss: 0.0340 - val_accuracy: 0.9722 - val_loss: 0.1138\n",
            "Epoch 64/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9931 - loss: 0.0240\n",
            "Epoch 64: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_64.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 117ms/step - accuracy: 0.9931 - loss: 0.0240 - val_accuracy: 0.9738 - val_loss: 0.1211\n",
            "Epoch 65/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9936 - loss: 0.0218\n",
            "Epoch 65: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_65.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9936 - loss: 0.0218 - val_accuracy: 0.9706 - val_loss: 0.1202\n",
            "Epoch 66/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9832 - loss: 0.0504\n",
            "Epoch 66: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_66.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9832 - loss: 0.0505 - val_accuracy: 0.9651 - val_loss: 0.1143\n",
            "Epoch 67/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9853 - loss: 0.0431\n",
            "Epoch 67: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_67.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9853 - loss: 0.0431 - val_accuracy: 0.9722 - val_loss: 0.1072\n",
            "Epoch 68/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9918 - loss: 0.0244\n",
            "Epoch 68: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_68.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9919 - loss: 0.0244 - val_accuracy: 0.9683 - val_loss: 0.1414\n",
            "Epoch 69/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9969 - loss: 0.0140\n",
            "Epoch 69: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_69.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 117ms/step - accuracy: 0.9969 - loss: 0.0139 - val_accuracy: 0.9762 - val_loss: 0.1105\n",
            "Epoch 70/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9977 - loss: 0.0070\n",
            "Epoch 70: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_70.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9977 - loss: 0.0071 - val_accuracy: 0.9770 - val_loss: 0.1189\n",
            "Epoch 71/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9953 - loss: 0.0135\n",
            "Epoch 71: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_71.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.9952 - loss: 0.0136 - val_accuracy: 0.9770 - val_loss: 0.1032\n",
            "Epoch 72/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9902 - loss: 0.0325\n",
            "Epoch 72: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_72.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.9902 - loss: 0.0325 - val_accuracy: 0.9730 - val_loss: 0.1436\n",
            "Epoch 73/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9936 - loss: 0.0259\n",
            "Epoch 73: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_73.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.9935 - loss: 0.0260 - val_accuracy: 0.9683 - val_loss: 0.1261\n",
            "Epoch 74/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9922 - loss: 0.0235\n",
            "Epoch 74: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_74.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9922 - loss: 0.0235 - val_accuracy: 0.9730 - val_loss: 0.1292\n",
            "Epoch 75/75\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9949 - loss: 0.0162\n",
            "Epoch 75: saving model to /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_75.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9949 - loss: 0.0162 - val_accuracy: 0.9762 - val_loss: 0.1229\n",
            "\n",
            "--- CRNN Model Training Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Diagonsis"
      ],
      "metadata": {
        "id": "WeCdWmsrcv60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration & Path Definitions (CRITICAL) ---\n",
        "\n",
        "# This path MUST match where you saved your TFLite model file\n",
        "TFLITE_MODEL_PATH = '/content/drive/MyDrive/Trained_Models/wildlife_crnn_quantized.tflite'\n",
        "# This path is used for saving the output image (adjust if necessary)\n",
        "BASE_MODELS_DIR = '/content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/'\n",
        "\n",
        "# NOTE: X_test_crnn, y_test_one_hot, and class_names are assumed to be defined\n",
        "# by the execution of the data splitting script provided previously.\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# --- 1. Load TFLite Model and Setup Interpreter ---\n",
        "\n",
        "# Initialize the TFLite Interpreter\n",
        "try:\n",
        "    interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    print(\"TFLite Interpreter loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR: Failed to load TFLite model. Please ensure the path is correct and the file exists. Error: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 2. Run Inference Sample by Sample ---\n",
        "\n",
        "num_test_samples = X_test_crnn.shape[0]\n",
        "num_classes = len(class_names)\n",
        "tflite_predictions = np.zeros((num_test_samples, num_classes), dtype=np.float32)\n",
        "\n",
        "print(f\"\\nStarting TFLite inference on {num_test_samples} test samples...\")\n",
        "\n",
        "for i in range(num_test_samples):\n",
        "    # Get one sample and ensure it has the required batch size of 1\n",
        "    input_data = X_test_crnn[i:i+1]\n",
        "\n",
        "    # Set the input tensor\n",
        "    # NOTE: If your TFLite model is quantized (INT8), you need to handle scaling here.\n",
        "    # Assuming float32 input for simplicity unless the model explicitly requires int8.\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.float32))\n",
        "\n",
        "    # Run the inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get the output probabilities\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    tflite_predictions[i] = output_data.squeeze()\n",
        "\n",
        "print(\"TFLite inference complete.\")\n",
        "\n",
        "\n",
        "# --- 3. Generate Classification Metrics ---\n",
        "\n",
        "# Convert predictions (probabilities) and true labels (one-hot) to integer indices\n",
        "predicted_classes = np.argmax(tflite_predictions, axis=1)\n",
        "true_classes = np.argmax(y_test_one_hot, axis=1)\n",
        "\n",
        "\n",
        "# A. Classification Report\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"           TFLITE CLASSIFICATION REPORT (Quantized Performance)\")\n",
        "print(\"=\"*70)\n",
        "# The target_names argument maps the integer indices back to your class names\n",
        "print(classification_report(true_classes, predicted_classes, target_names=class_names))\n",
        "\n",
        "\n",
        "# B. Confusion Matrix\n",
        "cm = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(f'Confusion Matrix for TFLITE CRNN Model (Edge Performance)')\n",
        "plt.ylabel('True Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Save the matrix image\n",
        "matrix_output_path = os.path.join(BASE_MODELS_DIR, 'confusion_matrix_tflite_quantized.png')\n",
        "plt.savefig(matrix_output_path)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nConfusion matrix image saved to: {matrix_output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mXtnjmyFczRK",
        "outputId": "60807f05-70e6-4587-9e47-e9de8609910e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFLite Interpreter loaded successfully.\n",
            "\n",
            "Starting TFLite inference on 1261 test samples...\n",
            "TFLite inference complete.\n",
            "\n",
            "======================================================================\n",
            "           TFLITE CLASSIFICATION REPORT (Quantized Performance)\n",
            "======================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Background       0.87      0.94      0.90       155\n",
            "        Bear       0.92      0.90      0.91       168\n",
            "        Deer       1.00      0.99      1.00       148\n",
            "    Elephant       0.99      0.96      0.98       162\n",
            "      Monkey       0.97      0.95      0.96       155\n",
            "     Peacock       0.99      0.96      0.98       151\n",
            "       Tiger       0.90      0.95      0.93       173\n",
            "   Wild Boar       0.99      0.97      0.98       149\n",
            "\n",
            "    accuracy                           0.95      1261\n",
            "   macro avg       0.96      0.95      0.95      1261\n",
            "weighted avg       0.95      0.95      0.95      1261\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1200 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion matrix image saved to: /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/confusion_matrix_tflite_quantized.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF to TFlite Converson"
      ],
      "metadata": {
        "id": "c4288hbrEgtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# 1. Load the model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_71.h5')\n",
        "\n",
        "# 2. Define the converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# 3. Apply Optimization (Quantization)\n",
        "# This reduces model size and speeds up inference on integer-only processors (CPUs).\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Note: For best results, use Representative Dataset Quantization,\n",
        "# but for simplicity, we start with default quantization.\n",
        "\n",
        "# --- Fix for the ConverterError related to TensorList and dynamic shapes ---\n",
        "# The error message suggests using SELECT_TF_OPS and disabling lower_tensor_list_ops\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Use TFLite built-in operations\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS     # Allow selecting TensorFlow operations\n",
        "]\n",
        "# Disable the experimental flag that causes issues with dynamic shapes in this case\n",
        "converter._experimental_lower_tensor_list_ops = False\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# 4. Convert the model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# 5. Save the TFLite model file\n",
        "tflite_path = '/content/drive/MyDrive/Trained_Models/wildlife_crnn_quantized.tflite' # Added a filename\n",
        "with open(tflite_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"Model successfully converted and saved as: {tflite_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZcEVIO8EgCW",
        "outputId": "43f58f84-b584-4e78-df3f-608896cd3448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmptu8cfc9a'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 173, 40, 1), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 8), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136389521045840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389521043728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507842704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507851152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507852688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507852496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507850768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507851344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507851728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507849616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507851536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507850960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507847504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507850384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507848848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507846544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507849232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507848464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507846160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507848080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507846736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136389507845008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Model successfully converted and saved as: /content/drive/MyDrive/Trained_Models/wildlife_crnn_quantized.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking input type of TFlite model\n"
      ],
      "metadata": {
        "id": "GM3bSvGi9T5a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c231c1b",
        "outputId": "6993cd5c-ae1c-474c-c220-37098dd0bb53"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the path to your TFLite model\n",
        "TFLITE_MODEL_PATH = '/content/drive/MyDrive/Trained_Models/wildlife_crnn_quantized.tflite'\n",
        "\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input details\n",
        "input_details = interpreter.get_input_details()\n",
        "\n",
        "# Check the data type of the input tensor\n",
        "input_dtype = input_details[0]['dtype']\n",
        "\n",
        "print(f\"The TFLite model expects input of type: {input_dtype}\")\n",
        "\n",
        "# You can compare this to expected types like tf.float32 or tf.int8\n",
        "if input_dtype == tf.float32:\n",
        "    print(\"The model accepts float32 input.\")\n",
        "elif input_dtype == tf.int8:\n",
        "    print(\"The model accepts int8 input.\")\n",
        "else:\n",
        "    print(f\"The model expects an unexpected input type: {input_dtype}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The TFLite model expects input of type: <class 'numpy.float32'>\n",
            "The model accepts float32 input.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "new tflite solve the error"
      ],
      "metadata": {
        "id": "4FTKAzBbKOyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# --- PATHS ---\n",
        "H5_MODEL_PATH = '/content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_71.h5'\n",
        "NEW_TFLITE_PATH = '/content/drive/MyDrive/Trained_Models/wildlife_crnn_quantized(float32).tflite'\n",
        "# ---\n",
        "\n",
        "print(f\"Loading Keras model from: {H5_MODEL_PATH}\")\n",
        "\n",
        "# 1. Load the model\n",
        "model = tf.keras.models.load_model(H5_MODEL_PATH)\n",
        "\n",
        "# 2. Define the converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# 3. Apply Optimization (Quantization)\n",
        "# --- THIS LINE IS REMOVED TO FIX THE FLEX OP CONFLICT ---\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# ---\n",
        "\n",
        "# 4. Fix for the ConverterError related to TensorList and dynamic shapes\n",
        "# This part is CORRECT and NECESSARY\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Use TFLite built-in operations\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS     # Allow selecting TensorFlow operations\n",
        "]\n",
        "converter._experimental_lower_tensor_list_ops = False\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "print(\"Converting model (float32) with Flex ops enabled...\")\n",
        "\n",
        "# 5. Convert the model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# 6. Save the TFLite model file\n",
        "with open(NEW_TFLITE_PATH, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"Model successfully converted and saved as: {NEW_TFLITE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SAHlKpcKNo3",
        "outputId": "2459403e-98b5-4a12-fd6b-530dcf700914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Keras model from: /content/drive/MyDrive/Trained_Models/CRNN_Checkpoints/crnn_checkpoint_71.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting model (float32) with Flex ops enabled...\n",
            "Saved artifact at '/tmp/tmpazzr__yt'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 173, 40, 1), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 8), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  133069595929168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069595929936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069595931472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069595932240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069595929552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069595930896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069595934160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069595934544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563495824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563496208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563495248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563494672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563498320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563496784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563495632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563499088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563494480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563496976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563497744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563497552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563497168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133069563501008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Model successfully converted and saved as: /content/drive/MyDrive/Trained_Models/wildlife_crnn_quantized(float32).tflite\n"
          ]
        }
      ]
    }
  ]
}