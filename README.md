# wildlife-Monitor
It is a project used to identify wild animals using their sound and images captured via mic and camera. The sound model and video model are integrated using the concept Decision-Level Fusion concept. It gives high priority to image model, that is if both model identified different animal we take the prediction done by the image model. If the identified animal belong to the threat class defined by the user the system plays a deterrent voice that make animal flee back to forest, but alert is made by the system for every animal identified.
# Sound Model
The sound model is bulit using the deep learning architecture CRNN.
1. Stage 1: The Feature Extractor (The CNN Block)

This first block of the CRNN is a Convolutional Neural Network (CNN). It is primarily responsible for turning the complex audio data into understandable visual patterns.

    Input Conversion: The sound is first converted into MFCC (Mel-Frequency Cepstral Coefficients) features, which are then arranged into a 2D grid (Time×Frequency) that the model treats like a grayscale image.

    Local Pattern Recognition: The Conv2D layers scan small sections of this MFCC grid. They learn to recognize local acoustic textures that are unique to certain sounds, regardless of when they occur in the 4-second clip.

2. Stage 2: The Sequence Analyzer (The GRU/LSTM Block)

This is the recurrent block that gives the model memory and allows it to understand the flow of the sound event over time.

    Conversion to Sequence: The 2D feature map from the CNN is restructured into a 1D sequence of vectors. Each vector represents the acoustic patterns detected during a brief moment in time (a time step).

    Temporal Analysis (Memory): The GRU (or LSTM) layers analyze this sequence step-by-step. It uses its internal memory to understand the "grammar" of the sound:

        It learns that a Tiger roar typically follows the sequence: [Sharp Onset] → [Low-Frequency Sustain] → [High-Frequency End].

        It learns that an Elephant alarm follows: [Sustained, Rising Volume] → [Sharp Trumpet Peak].

    Context Vector: The final GRU layer summarizes the entire 4-second sequence into a single, comprehensive context vector. This vector contains the model's ultimate mathematical summary of the sound event.

3. Final Decision

The final Dense layers take the context vector and map it to your 8 animal classes. By studying the data this way, the CRNN is highly effective because it ensures the sound must possess both the correct texture (CNN) and the correct timing (GRU) to be correctly classified.

Animals used in sound model: Elephant,Tiger,Bear,Deer,Monkey,Wild Boar,Peacock,Background(noises that should be ignored by the model)

# Image Model
The image model is done using an EfficientNET model.
1. Multi-Stage Feature Extraction
   Initial Processing (stem_conv): The model first analyzes the image to find very simple, low-level features like edges, lines, and basic color gradients.

    Deep Feature Extraction (block1a,block2a,block3a,etc.): The model then passes the data through many sequential blocks. Each block learns more complex patterns: the first blocks learn textures (like fur or stripes), and the later blocks learn spatial relationships (like "head shape" or "four legs").
2. The Core Learning Mechanism: MBConv Blocks

The foundational unit of EfficientNet is the Mobile Inverted Bottleneck Convolution (MBConv) Block. This block is crucial for efficient learning and is identifiable by the layer names in your summary:

    Depthwise Convolution (block_dwconv): Unlike a traditional CNN that mixes all color channels together, the Depthwise Convolution separates the spatial filtering from the channel filtering. This drastically reduces the number of parameters and speeds up the model.
    

    Squeeze-and-Excitation (SE) Module (block_se_squeeze): This is the brain of the block. It looks at all the feature maps generated by the convolution and assigns a weight to each channel.

        Example: When classifying a Leopard, the SE module might assign a high weight to the "spotted texture" channel and a low weight to the "solid color" channel, ensuring the model focuses its attention on the most discriminative feature (the spots).

3. The Final Prediction

After the data passes through all the convolutional blocks, it reaches the custom layers you added:

    GlobalAveragePooling: This layer collapses the 2D image data into a single, comprehensive 1D Feature  Vector (e.g., 1280 numbers) that summarizes the entire image.

    Custom Dense Head (dense_1,predictions): Your new Dense layers take this feature vector and map it directly to your 9 animal  classes. This final step determines the prediction and its confidence score.

# Deterrent voice
A voice generated with white noise and sine wave of specific frequency using audacity.

** Further detais about the project given in the project report **
